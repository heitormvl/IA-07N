{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28315909",
   "metadata": {},
   "source": [
    "## 1. Instalar e importar pacotes necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn statsmodels scikit-learn tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "# Modelos e métricas\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Configurações de visualização\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02eeae",
   "metadata": {},
   "source": [
    "## 2. Carregar o arquivo `paysim.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo CSV\n",
    "csv_path = \"paysim.csv\"\n",
    "\n",
    "# Carrega o DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Visualiza as primeiras linhas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec648441",
   "metadata": {},
   "source": [
    "## 3. Tratamento dos dados antes do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Informações gerais e detecção de valores ausentes\n",
    "print(\"Dimensões do dataset:\", df.shape)\n",
    "print(\"Contagem de valores ausentes por coluna:\\n\", df.isna().sum())\n",
    "print(\"\\nTipos de dados:\\n\", df.dtypes)\n",
    "\n",
    "# 3.2. Remover duplicatas (se existirem)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# 3.3. Converter coluna `step` em índice de tempo\n",
    "# O campo `step` indica o número de horas desde o início da simulação.\n",
    "# Vamos assumir que a origem seja agora e converter para um índice de data.\n",
    "now = pd.Timestamp.now()\n",
    "df[\"date\"] = pd.to_datetime(df[\"step\"], unit=\"h\", origin=now)\n",
    "\n",
    "# Definir `date` como índice\n",
    "df.set_index(\"date\", inplace=True)\n",
    "\n",
    "# 3.4. Converter colunas numéricas para tipo adequado (float)\n",
    "numeric_cols = [\n",
    "    \"amount\",\n",
    "    \"oldbalanceOrg\",\n",
    "    \"newbalanceOrig\",\n",
    "    \"oldbalanceDest\",\n",
    "    \"newbalanceDest\",\n",
    "]\n",
    "df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "# 3.5. Filtrar tipos de transações que interessam para previsão de volume\n",
    "# (por exemplo, apenas transações de tipo 'PAYMENT' e 'TRANSFER')\n",
    "types_to_keep = [\"PAYMENT\", \"TRANSFER\"]\n",
    "df_ts = df[df[\"type\"].isin(types_to_keep)].copy()\n",
    "\n",
    "# 3.6. Agregar valores por hora (soma de `amount`)\n",
    "hourly_amount = (\n",
    "    df_ts[\"amount\"]\n",
    "    .resample(\"H\")  # agregação horária\n",
    "    .sum()\n",
    "    .rename(\"total_amount\")\n",
    "    .to_frame()\n",
    ")\n",
    "\n",
    "\n",
    "# 3.7. Visualizar a série temporal agregada\n",
    "plt.figure(figsize=(12, 4))\n",
    "hourly_amount[\"total_amount\"].plot()\n",
    "plt.title(\"Série Temporal Horária de Volume de Transações (PaySim)\")\n",
    "plt.ylabel(\"Soma horária de `amount`\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.8. Verificar estacionariedade inicial (ADF test)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result_adf = adfuller(hourly_amount[\"total_amount\"].dropna())\n",
    "print(\"ADF Statistic: {:.4f}\".format(result_adf[0]))\n",
    "print(\"p-value: {:.4f}\".format(result_adf[1]))\n",
    "for key, value in result_adf[4].items():\n",
    "    print(\"Critério {}: {:.4f}\".format(key, value))\n",
    "\n",
    "# 3.9. Caso necessário, aplicar diferenciação (1ª diferença)\n",
    "hourly_amount[\"diff_1\"] = hourly_amount[\"total_amount\"].diff()\n",
    "\n",
    "# 3.10. Remover eventuais NaNs gerados pela diferenciação\n",
    "hourly_amount.dropna(inplace=True)\n",
    "\n",
    "# 3.11. Plot da série diferenciada para verificar estacionariedade\n",
    "plt.figure(figsize=(12, 4))\n",
    "hourly_amount[\"diff_1\"].plot()\n",
    "plt.title(\"Série Horária Diferenciada (1ª diferença)\")\n",
    "plt.ylabel(\"Diferença de `total_amount`\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.12. Verificar estacionariedade da série diferenciada\n",
    "result_adf_diff = adfuller(hourly_amount[\"diff_1\"])\n",
    "print(\"ADF Statistic (differenced): {:.4f}\".format(result_adf_diff[0]))\n",
    "print(\"p-value (differenced): {:.4f}\".format(result_adf_diff[1]))\n",
    "for key, value in result_adf_diff[4].items():\n",
    "    print(\"Critério {}: {:.4f}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c139ba",
   "metadata": {},
   "source": [
    "**Análise dos Dados e Estacionariedade (Granularidade Horária)**\n",
    "\n",
    "1. **Dimensões e Integridade**\n",
    "\n",
    "   * O DataFrame contém **6.362.620 linhas e 11 colunas**.\n",
    "   * Não há valores ausentes em nenhuma coluna — todas têm contagem zero de `isna()`.\n",
    "   * Tipos de dados:\n",
    "\n",
    "     * `step`: `int64`\n",
    "     * `type`, `nameOrig`, `nameDest`: `object` (strings)\n",
    "     * `amount`, `oldbalanceOrg`, `newbalanceOrig`, `oldbalanceDest`, `newbalanceDest`: `float64`\n",
    "     * `isFraud`, `isFlaggedFraud`: `int64`\n",
    "\n",
    "2. **Conversão de `step` para Índice Temporal**\n",
    "\n",
    "   * A coluna `step` (horas desde “agora”) foi convertida em `datetime` usando:\n",
    "\n",
    "     ```python\n",
    "     now = pd.Timestamp.now()\n",
    "     df[\"date\"] = pd.to_datetime(df[\"step\"], unit=\"h\", origin=now)\n",
    "     df.set_index(\"date\", inplace=True)\n",
    "     ```\n",
    "   * O índice `date` foi definido sem gerar valores ausentes, preservando todas as 6.362.620 linhas.\n",
    "\n",
    "3. **Filtragem e Agregação Horária**\n",
    "\n",
    "   * Selecionaram-se transações dos tipos **`PAYMENT`** e **`TRANSFER`** para focar na previsão de volume relevante:\n",
    "\n",
    "     python\n",
    "     types_to_keep = [\"PAYMENT\", \"TRANSFER\"]\n",
    "     df_ts = df[df[\"type\"].isin(types_to_keep)].copy()\n",
    "     \n",
    "   * Em seguida, agregou-se o atributo `amount` por hora (soma de todas as transações em cada hora):\n",
    "\n",
    "     python\n",
    "     hourly_amount = (\n",
    "         df_ts[\"amount\"]\n",
    "         .resample(\"H\")      # agregação horária\n",
    "         .sum()\n",
    "         .rename(\"total_amount\")\n",
    "         .to_frame()\n",
    "     )\n",
    "     \n",
    "   * O gráfico abaixo mostra a **Série Temporal Horária de Volume de Transações** (PaySim) ao longo de um mês:\n",
    "\n",
    "4. **Teste de Estacionariedade (ADF) na Série Original**\n",
    "\n",
    "   * Aplicou-se o teste de Dickey–Fuller (ADF) diretamente sobre a série horária agregada:\n",
    "\n",
    "     python\n",
    "     from statsmodels.tsa.stattools import adfuller\n",
    "     result_adf = adfuller(hourly_amount[\"total_amount\"].dropna())\n",
    "     print(\"ADF Statistic: {:.4f}\".format(result_adf[0]))\n",
    "     print(\"p-value: {:.4f}\".format(result_adf[1]))\n",
    "     for key, value in result_adf[4].items():\n",
    "         print(\"Critério {}: {:.4f}\".format(key, value))\n",
    "     \n",
    "   * **Resultados obtidos**:\n",
    "\n",
    "     * **Estatística ADF**: −2.6928\n",
    "     * **p-value**: 0.0753\n",
    "     * **Valores Críticos**:\n",
    "\n",
    "       * 1%: −3.4394\n",
    "       * 5%: −2.8655\n",
    "       * 10%: −2.5689\n",
    "   * **Interpretação**:\n",
    "\n",
    "     * Como **p-value (0.0753) > 0.05** e o valor ADF (−2.6928) está acima do crítico a 5% (−2.8655), não rejeitamos a hipótese de raiz unitária para a série original.\n",
    "     * Logo, a **série horária de `total_amount` não é estacionária**.\n",
    "\n",
    "5. **Primeira Diferença e Novo Teste ADF**\n",
    "\n",
    "   * Calculou-se a primeira diferença para remover tendência de nível:\n",
    "\n",
    "     python\n",
    "     hourly_amount[\"diff_1\"] = hourly_amount[\"total_amount\"].diff()\n",
    "     hourly_amount.dropna(inplace=True)\n",
    "     \n",
    "   * Aplicou-se o ADF sobre a série diferenciada (`diff_1`):\n",
    "\n",
    "     python\n",
    "     result_adf_diff = adfuller(hourly_amount[\"diff_1\"])\n",
    "     print(\"ADF Statistic (differenced): {:.4f}\".format(result_adf_diff[0]))\n",
    "     print(\"p-value (differenced): {:.4f}\".format(result_adf_diff[1]))\n",
    "     for key, value in result_adf_diff[4].items():\n",
    "         print(\"Critério {}: {:.4f}\".format(key, value))\n",
    "     \n",
    "   * **Resultados obtidos (differenced)**:\n",
    "\n",
    "     * **Estatística ADF (differenced)**: −14.4205\n",
    "     * **p-value (differenced)**: 0.0000\n",
    "     * **Valores Críticos (differenced)**:\n",
    "\n",
    "       * 1%: −3.4395\n",
    "       * 5%: −2.8656\n",
    "       * 10%: −2.5689\n",
    "   * **Interpretação**:\n",
    "\n",
    "     * Como **p-value (0.0000) < 0.05** e ADF (−14.4205) é muito menor que o crítico a 1% (−3.4395), rejeitamos a hipótese de raiz unitária na série diferenciada.\n",
    "     * Portanto, a **primeira diferença (`diff_1`) é estacionária**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusão**\n",
    "\n",
    "* A **série original de “soma horária de `amount`”** é **não estacionária** (confirmado pelo ADF original).\n",
    "* A **primeira diferença** (`diff_1`) torna a série **estacionária**, conforme o ADF diferenciado.\n",
    "* Para ajustar um modelo ARIMA adequadamente, deve-se usar:\n",
    "\n",
    "  * **Série diferenciada** com `d=1` (por ex. `ARIMA(p, 1, q)`),\n",
    "  * Ou incorporar diretamente a diferença no próprio ARIMA (`d=1`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9e9c8",
   "metadata": {},
   "source": [
    "## 4. Construir, Treinar e Validar o Modelo ARIMA\n",
    "\n",
    "A seguir, vamos ajustar um modelo ARIMA(p, 1, q) sobre a série original de soma horária de `amount`, indicando `d = 1` para incorporar a primeira diferença que analisamos anteriormente. Para este exemplo, adotaremos inicialmente $p = 1$ e $q = 1$.\n",
    "Em seguida, faremos um split da série em conjunto de treino e teste, ajustaremos o modelo, geraremos previsões e avaliaremos o desempenho usando o MSE e gráficos de comparação.\n",
    "\n",
    "### 4.1. Divisão entre Treino e Teste\n",
    "\n",
    "Vamos usar os primeiros 80 % dos pontos para treino e os 20 % finais para teste, preservando a ordem temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir divisão em 80/20\n",
    "\n",
    "n = len(hourly_amount)\n",
    "train_size = int(n * 0.8)\n",
    "\n",
    "train_series = hourly_amount[\"total_amount\"].iloc[:train_size]\n",
    "test_series = hourly_amount[\"total_amount\"].iloc[train_size:]\n",
    "\n",
    "print(f\"Número de pontos - Total: {n}, Treino: {len(train_series)}, Teste: {len(test_series)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f398d27",
   "metadata": {},
   "source": [
    "### 4.2. Ajuste do Modelo ARIMA(1, 1, 1)\n",
    "\n",
    "Ajustaremos `order=(1, 1, 1)` diretamente sobre a série original (o `d=1` já incorpora a primeira diferença internamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar o modelo ARIMA(1,1,1) nos dados de treino\n",
    "\n",
    "model = ARIMA(train_series, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f8028",
   "metadata": {},
   "source": [
    "### 4.3. Geração de Previsões no Conjunto de Teste\n",
    "\n",
    "Vamos gerar previsões para todo o período de teste e comparar com os valores observados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0474931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsão para o período de teste\n",
    "\n",
    "start = test_series.index[0]\n",
    "end = test_series.index[-1]\n",
    "\n",
    "forecast = model_fit.predict(start=start, end=end, typ=\"levels\")\n",
    "\n",
    "# Garantir alinhamento de índice\n",
    "\n",
    "forecast = pd.Series(forecast, index=test_series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae34832",
   "metadata": {},
   "source": [
    "### 4.4. Avaliação de Desempenho (MSE) e Gráfico Comparativo\n",
    "\n",
    "Calculamos o MSE entre as previsões e os valores reais, e plotamos ambos para análise visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do MSE\n",
    "\n",
    "mse = mean_squared_error(test_series, forecast)\n",
    "print(f\"Mean Squared Error no conjunto de teste: {mse:.2f}\")\n",
    "\n",
    "# Plotando série real vs previsão\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_series.index, train_series, label=\"Treino (total_amount)\")\n",
    "plt.plot(test_series.index, test_series, label=\"Teste (total_amount)\", color=\"orange\")\n",
    "plt.plot(forecast.index, forecast, label=\"Previsão ARIMA(1,1,1)\", color=\"green\", linestyle=\"--\")\n",
    "plt.axvline(test_series.index[0], color=\"gray\", linestyle=\":\", linewidth=1)\n",
    "plt.legend()\n",
    "plt.title(\"ARIMA(1,1,1): Série Real vs Previsão\")\n",
    "plt.ylabel(\"Soma horária de amount\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3af4c",
   "metadata": {},
   "source": [
    "# INSERIR CONCLUSÃO ARIMA AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd0ca0",
   "metadata": {},
   "source": [
    "## 5. Desenvolvimento do Modelo LSTM para Previsão de Volume\n",
    "\n",
    "### 5.1. Preparação dos Dados para LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como já temos a série horária de total_amount (hourly_amount)\n",
    "# Vamos usar a coluna \"total_amount\" para construir exemplos sequenciais\n",
    "\n",
    "# 5.1.1. Selecionar a série original\n",
    "series = hourly_amount[\"total_amount\"].values.reshape(-1, 1)\n",
    "\n",
    "# 5.1.2. Escalonamento entre 0 e 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "series_scaled = scaler.fit_transform(series)\n",
    "\n",
    "\n",
    "# 5.1.3. Função auxiliar para criar janelas (timesteps) de tamanho look_back\n",
    "def create_sequences(data, look_back=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i : i + look_back, 0])\n",
    "        y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# 5.1.4. Definir tamanho da janela (e.g., últimas 24 horas)\n",
    "look_back = 24\n",
    "\n",
    "# 5.1.5. Criar sequências\n",
    "X_all, y_all = create_sequences(series_scaled, look_back)\n",
    "\n",
    "# 5.1.6. Remodelar X para o formato [amostras, timesteps, features]\n",
    "X_all = X_all.reshape((X_all.shape[0], look_back, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef526e",
   "metadata": {},
   "source": [
    "### 5.2. Divisão Treino / Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar os mesmos índices de treino/teste definidos para ARIMA\n",
    "n_total = X_all.shape[0]\n",
    "train_size = int(n_total * 0.8)\n",
    "\n",
    "X_train = X_all[:train_size]\n",
    "y_train = y_all[:train_size]\n",
    "X_test = X_all[train_size:]\n",
    "y_test = y_all[train_size:]\n",
    "\n",
    "print(f\"Amostras - Total: {n_total}, Treino: {len(X_train)}, Teste: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82708800",
   "metadata": {},
   "source": [
    "### 5.3. Construção da Rede LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2fb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(\n",
    "    LSTM(\n",
    "        units=50,\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        return_sequences=True,\n",
    "    )\n",
    ")\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(units=50, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(units=1))\n",
    "\n",
    "model_lstm.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6a257",
   "metadata": {},
   "source": [
    "### 5.4. Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2affdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_lstm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5696f91",
   "metadata": {},
   "source": [
    "### 5.5. Previsão e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd338eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.1. Previsão no conjunto de teste\n",
    "pred_scaled = model_lstm.predict(X_test)\n",
    "pred = scaler.inverse_transform(pred_scaled.reshape(-1, 1))\n",
    "real = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# 5.5.2. Cálculo do MSE\n",
    "mse_lstm = mean_squared_error(real, pred)\n",
    "print(f\"MSE LSTM no conjunto de teste: {mse_lstm:.2f}\")\n",
    "\n",
    "# 5.5.3. Plot real vs previsão\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(\n",
    "    hourly_amount.index[train_size + look_back : train_size + look_back + len(real)],\n",
    "    real,\n",
    "    label=\"Real (total_amount)\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "plt.plot(\n",
    "    hourly_amount.index[train_size + look_back : train_size + look_back + len(pred)],\n",
    "    pred,\n",
    "    label=\"Previsão LSTM\",\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "plt.title(\"LSTM: Série Real vs Previsão\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Soma horária de amount\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd2da7",
   "metadata": {},
   "source": [
    "### 5.6. Análise dos Resultados do LSTM\n",
    "\n",
    "Ao observar o gráfico “LSTM: Série Real vs Previsão” e o MSE obtido, podemos destacar:\n",
    "\n",
    "1. **MSE Elevado nas Unidades Originais**  \n",
    "   - O MSE no conjunto de teste ficou em aproximadamente **1,9228×10¹⁶** (19227631393269504).  \n",
    "   - Esse valor alto evidencia que, em termos absolutos (quando transformamos de volta para a escala original), há desvio significativo em algumas horas, principalmente nos picos extremos.\n",
    "\n",
    "2. **Captura de Tendência Geral**  \n",
    "   - O modelo LSTM segue razoavelmente a forma geral da série: consegue identificar momentos de crescimento e queda no volume horáriodo dataset.  \n",
    "   - Em períodos de variação moderada, a curva de previsão acompanha de perto os valores reais, contribuindo para um erro médio aceitável em horas sem picos.\n",
    "\n",
    "3. **Subestimação de Picos Extremos**  \n",
    "   - Nas horas de pico (valores muito elevados), o LSTM tende a “suavizar” a previsão.  \n",
    "   - Por exemplo, no dia 30 de junho, quando a série real ultrapassa 1×10⁹, a previsão fica em torno de 4×10⁸, criando grande discrepância para essas poucas amostras e elevando o MSE geral.\n",
    "\n",
    "4. **Defasagem Temporária em Mudanças Abruptas**  \n",
    "   - Em regiões de aumento rápido ou queda acentuada no volume, observa-se leve atraso na resposta do LSTM.  \n",
    "   - Essa defasagem é esperada em redes recorrentes que utilizam informações passadas (janela de 24 horas) para projetar o próximo passo.\n",
    "\n",
    "5. **Implicações Práticas**  \n",
    "   - Para tarefas em que interessa capturar a tendência geral do volume (por exemplo, planejamento de capacidade ou detecção de sazonalidades), o desempenho do LSTM é satisfatório.  \n",
    "   - Entretanto, se o objetivo for detectar ou prever picos anômalos (como grandes volumes atípicos que podem estar associados a episódios de fraude), o modelo, do jeito atual, não é suficientemente sensível a esses extremos.\n",
    "\n",
    "6. **Possíveis Ajustes e Melhorias**  \n",
    "   - **Arquitetura e Hiperparâmetros:** Aumentar/decrementar unidades LSTM, camadas, taxa de dropout ou epochs pode ajudar a modelar melhor variações abruptas.  \n",
    "   - **Features Adicionais:** Incluir indicadores de horário do dia, dia da semana ou outras covariáveis externas pode dar mais contexto ao LSTM.  \n",
    "   - **Modelos Híbridos:** Combinar LSTM (para capturar tendências complexas) com ARIMA ou modelos baseados em atenção pode melhorar a resposta a picos.  \n",
    "   - **Ajuste de Janela:** Testar diferentes tamanhos de janela (look_back) para equilibrar a memória do modelo entre padrão geral e eventos pontuais.\n",
    "\n",
    "Em resumo, o LSTM apresentou boa capacidade de seguir as tendências gerais do volume horáriodo PaySim, mas mostrou limitações ao estimar picos extremos, refletidas no MSE elevado (∼1,92×10¹⁶)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d8973",
   "metadata": {},
   "source": [
    "### 5.7 Suavização das Séries para nova Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c57b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_amount[\"total_amount_log\"] = np.log1p(hourly_amount[\"total_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0491a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7.1. Selecionar a série original\n",
    "series = hourly_amount[\"total_amount_log\"].values.reshape(-1, 1)\n",
    "\n",
    "# 5.7.2. Escalonamento entre 0 e 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "series_scaled = scaler.fit_transform(series)\n",
    "\n",
    "\n",
    "# 5.7.3. Função auxiliar para criar janelas (timesteps) de tamanho look_back\n",
    "def create_sequences(data, look_back=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i : i + look_back, 0])\n",
    "        y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# 5.7.4. Definir tamanho da janela (e.g., últimas 24 horas)\n",
    "look_back = 24\n",
    "\n",
    "# 5.7.5. Criar sequências\n",
    "X_all, y_all = create_sequences(series_scaled, look_back)\n",
    "\n",
    "# 5.7.6. Remodelar X para o formato [amostras, timesteps, features]\n",
    "X_all = X_all.reshape((X_all.shape[0], look_back, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar os mesmos índices de treino/teste definidos para ARIMA\n",
    "n_total = X_all.shape[0]\n",
    "train_size = int(n_total * 0.8)\n",
    "\n",
    "X_train = X_all[:train_size]\n",
    "y_train = y_all[:train_size]\n",
    "X_test = X_all[train_size:]\n",
    "y_test = y_all[train_size:]\n",
    "\n",
    "print(f\"Amostras - Total: {n_total}, Treino: {len(X_train)}, Teste: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c79f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(\n",
    "    LSTM(\n",
    "        units=50,\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        return_sequences=True,\n",
    "    )\n",
    ")\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(units=50, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(units=1))\n",
    "\n",
    "model_lstm.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ba0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_lstm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd539b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scaled = model_lstm.predict(X_test)\n",
    "pred = scaler.inverse_transform(pred_scaled.reshape(-1, 1))\n",
    "real = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "mse_lstm = mean_squared_error(real, pred)\n",
    "print(f\"MSE LSTM no conjunto de teste: {mse_lstm:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(\n",
    "    hourly_amount.index[train_size + look_back : train_size + look_back + len(real)],\n",
    "    real,\n",
    "    label=\"Real (total_amount)\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "plt.plot(\n",
    "    hourly_amount.index[train_size + look_back : train_size + look_back + len(pred)],\n",
    "    pred,\n",
    "    label=\"Previsão LSTM\",\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "plt.title(\"LSTM: Série Real vs Previsão\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Soma horária de amount\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d920712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_scaled: array (prediction) resultante do model.predict(X_test_log)\n",
    "pred_log = scaler.inverse_transform(pred_scaled.reshape(-1, 1))\n",
    "\n",
    "pred_original = np.expm1(pred_log)\n",
    "\n",
    "real_log = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "real_original = np.expm1(real_log)\n",
    "\n",
    "# Plotando a previsão com a série original\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(\n",
    "    hourly_amount.index[train_size + look_back : train_size + look_back + len(real_original)],\n",
    "    real_original,\n",
    "    label=\"Real (total_amount)\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "plt.plot(\n",
    "    hourly_amount.index[train_size + look_back : train_size + look_back + len(pred_original)],\n",
    "    pred_original,\n",
    "    label=\"Previsão LSTM (transformada de volta)\",\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "plt.title(\"LSTM: Série Real vs Previsão (Transformada de Volta)\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Soma horária de amount\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40735c29",
   "metadata": {},
   "source": [
    "### 5.8. Conclusão LSTM em previsão\n",
    "\n",
    "Após comparar as duas abordagens de LSTM—uma treinada diretamente na série de `total_amount` (sem transformação) e outra treinada na escala log (`log1p`) com posterior retorno à escala original (`expm1`)—podemos concluir:\n",
    "\n",
    "1. **Modelagem sem log**  \n",
    "   - O modelo capturou a ordem de grandeza dos picos (10⁸–10⁹), mas apresentou erros muito elevados (MSE ≈ 1,9×10¹⁶) causados pelos poucos eventos de volume extremo.  \n",
    "   - Houve bastante ruído em regiões de valor baixo, tornando o aprendizado instável e a previsão muito sensível aos outliers.\n",
    "\n",
    "2. **Modelagem com log + expm1**  \n",
    "   - O treinamento em escala log estabilizou os gradientes e reduziu a influência dos picos extremos no erro, gerando previsões mais suaves para valores médios e baixos.  \n",
    "   - Ao retransformar para a escala original, os picos maiores foram subestimados (por exemplo, um real de 10⁹ virou previsão na faixa de 10⁷–10⁸), mas o modelo ficou mais consistente no dia a dia e com menor variação excessiva.  \n",
    "\n",
    "3. **Recomendações**  \n",
    "   - Se o objetivo for **acompanhar padrões gerais** de volume e ter um modelo mais robusto à presença de poucos picos extremos, adote a versão com **log1p + expm1**. Dessa forma, o LSTM será treinado em uma faixa mais “achatada” e, ao inverter a transformação, teremos previsões razoáveis para a maior parte dos valores, mesmo que não capture integralmente cada pico.  \n",
    "   - Se for crucial identificar e prever com precisão a **magnitude exata dos picos** (por exemplo, em cenários onde cada grande transação representa potencial fraude), talvez seja necessário combinar o LSTM (em log) com um método específico de detecção de outliers ou então complementar com um modelo ARIMA/híbrido capaz de focar nos picos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa4ed9",
   "metadata": {},
   "source": [
    "## 6. Classificação de Transações (LSTM) Usando o Forecast de Volume como Critério\n",
    "\n",
    "Nesta etapa, vamos construir um modelo LSTM que **classifique cada transação** como fraudulenta ou não, incorporando como feature a previsão de volume horário gerada pelo modelo de previsão anterior.  \n",
    "Usaremos o mesmo dataset PaySim do Kaggle (coluna `isFraud`) para supervisionar o treinamento. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e290f",
   "metadata": {},
   "source": [
    "### 6.1. Objetivo e Visão Geral\n",
    "\n",
    "1. **Objetivo**: Treinar um classificador LSTM que, para cada transação, combine:\n",
    "   - Features da própria transação (_amount_, _tipo_, _saldos_, etc.),\n",
    "   - Previsão do **volume agregado horário** (obtida no passo 5),  \n",
    "   e então preveja `isFraud` (fraude ou não).  \n",
    "\n",
    "2. **Fluxo geral**:\n",
    "   1. Carregar o DataFrame original de transações (`df`).\n",
    "   2. Relembrar a série horária (`hourly_amount`) e o modelo de previsão baseado em LSTM (já treinado em 5.7).\n",
    "   3. Gerar a **coluna “forecast_volume”** (previsão de soma horária de `amount`) para cada hora.\n",
    "   4. Agregar essa previsão de volume de volta a cada linha de `df`, mapeando pela hora da transação.\n",
    "   5. Construir novas features de input – dentre elas:  \n",
    "      - `amount`, `type` (one-hot), `oldbalanceOrg`, `newbalanceOrig`, `oldbalanceDest`, `newbalanceDest`,  \n",
    "      - hora do dia, dia da semana, `forecast_volume` da hora correspondente,  \n",
    "      - talvez `residual = (volume_real_hora – forecast_volume)`, e etc.  \n",
    "   6. Criar sequências de tamanho _look_back_ de transações (por ordem cronológica) para alimentar o LSTM.\n",
    "   7. Definir e treinar o modelo LSTM “classificador” (com saída sigmóide).\n",
    "   8. Avaliar desempenho (Acurácia, AUC, Confusion Matrix).\n",
    "\n",
    "> **Pré-requisito**: As variáveis `hourly_amount`, `model_lstm` e `scaler` geradas no passo 5.7 devem estar em memória.  \n",
    "> - `hourly_amount` contém as colunas:  \n",
    ">   - `total_amount` (soma horária original),  \n",
    ">   - `total_amount_log` (log1p),  \n",
    ">   - `diff_1` (primeira diferença), etc.  \n",
    "> - `model_lstm` e `scaler` são o LSTM (em escala log) treinado no passo 5.7 e o `MinMaxScaler` usado para a série log.  \n",
    "> - `look_back` é o número de timestamps para cada amostra (por ex., 24).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2. Carregar o DataFrame original de transações PaySim (se necessário)\n",
    "#     - Caso já esteja em memória, basta pular esta célula.\n",
    "\n",
    "csv_path = \"paysim.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 6.2.1. Converter coluna \"step\" para datetime (mesmo procedimento do passo anterior)\n",
    "now = pd.Timestamp.now()\n",
    "df[\"date\"] = pd.to_datetime(df[\"step\"], unit=\"h\", origin=now)\n",
    "df.set_index(\"date\", inplace=True)\n",
    "\n",
    "# 6.2.2. Garantir ordenação cronológica\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# 6.2.3. Visualizar algumas linhas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7f2ec",
   "metadata": {},
   "source": [
    "### 6.3. Gerar previsões de volume horário usando o LSTM treinado (passo 5.7)\n",
    "\n",
    "1. Recuperamos a série `hourly_amount[\"total_amount_log\"]`.  \n",
    "2. Aplicamos o `scaler` (que já foi ajustado em 5.7) para transformar em escala `[0,1]`.  \n",
    "3. Construímos janelas de tamanho `look_back` para todas as horas possíveis.  \n",
    "4. Chamamos `model_lstm.predict(...)` para obter previsões em escala log.  \n",
    "5. Voltamos da escala log para o valor original com `expm1(...)`.  \n",
    "6. Associamos cada previsão de volta a um índice horário (as primeiras `look_back` horas não têm previsão).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9eefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3.1. Recuperar a série logarítmica e o scaler/modelo treinado\n",
    "#           (se você não tiver executado 5.7 há pouco, reexecute aquele bloco para restaurar essas variáveis)\n",
    "\n",
    "# hourly_amount (já contém \"total_amount_log\" do passo 5.7)\n",
    "# scaler (MinMaxScaler ajustado sobre total_amount_log)\n",
    "# model_lstm (LSTM treinado sobre séries log)\n",
    "\n",
    "# 6.3.2. Extrair array em log e escalar\n",
    "series_log = hourly_amount[\"total_amount_log\"].values.reshape(-1, 1)\n",
    "series_log_scaled = scaler.transform(series_log)  # usa o scaler de 5.7\n",
    "\n",
    "# 6.3.3. Construir X_all (janelas de tamanho look_back)\n",
    "X_all = []\n",
    "for i in range(len(series_log_scaled) - look_back):\n",
    "    X_all.append(series_log_scaled[i : i + look_back, 0])\n",
    "X_all = np.array(X_all).reshape((-1, look_back, 1))\n",
    "\n",
    "# 6.3.4. Fazer predição em escala log\n",
    "pred_scaled = model_lstm.predict(X_all, verbose=0)\n",
    "\n",
    "# 6.3.5. Voltar da escala MinMax para o valor log\n",
    "pred_log = scaler.inverse_transform(pred_scaled)\n",
    "\n",
    "# 6.3.6. Voltar do log1p para a escala original\n",
    "pred_original = np.expm1(pred_log)\n",
    "\n",
    "# 6.3.7. Construir uma série pandas com índice horário:\n",
    "#          o primeiro índice de previsão corresponde a hourly_amount.index[look_back]\n",
    "forecast_volume = pd.Series(\n",
    "    data=pred_original.flatten(),\n",
    "    index=hourly_amount.index[look_back:],\n",
    "    name=\"forecast_volume\",\n",
    ")\n",
    "\n",
    "# 6.3.8. Transformar em DataFrame para facilitar merge\n",
    "hourly_forecast = forecast_volume.to_frame()\n",
    "hourly_forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e51ea6",
   "metadata": {},
   "source": [
    "### 6.4. Mapear previsão horária a cada transação no DataFrame original\n",
    "\n",
    "1. A coluna `hourly_forecast[\"forecast_volume\"]` está indexada por timestamp a cada hora (por ex., 2025-06-30 14:00:00).  \n",
    "2. Cada transação em `df` possui um timestamp preciso (por ex., 2025-06-30 14:37:00).  \n",
    "3. Para unir, vamos “arredondar para baixo” cada timestamp de transação para a hora exata:  \n",
    "```python\n",
    "   df[\"hour\"] = df.index.floor(\"H\")\n",
    "````\n",
    "\n",
    "4. Em seguida, fazemos um merge com `hourly_forecast` usando `left_on=\"hour\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4.1. Criar coluna \"hour\" em df (timestamp arredondado para hora)\n",
    "df[\"hour\"] = df.index.floor(\"H\")\n",
    "\n",
    "# 6.4.2. Fazer merge left com hourly_forecast\n",
    "df = df.merge(hourly_forecast, how=\"left\", left_on=\"hour\", right_index=True)\n",
    "\n",
    "# 6.4.3. Verificar se há horas sem previsão (as primeiras look_back horas):\n",
    "#         nestes casos, podemos preencher com 0 ou com o valor médio de volume\n",
    "df[\"forecast_volume\"].fillna(0.0, inplace=True)\n",
    "\n",
    "# 6.4.4. Remover coluna auxiliar \"hour\"\n",
    "df.drop(columns=[\"hour\"], inplace=True)\n",
    "\n",
    "# 6.4.5. Visualizar como ficou (as colunas relevantes)\n",
    "df[\n",
    "    [\n",
    "        \"type\",\n",
    "        \"amount\",\n",
    "        \"oldbalanceOrg\",\n",
    "        \"newbalanceOrig\",\n",
    "        \"oldbalanceDest\",\n",
    "        \"newbalanceDest\",\n",
    "        \"forecast_volume\",\n",
    "        \"isFraud\",\n",
    "    ]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b730e",
   "metadata": {},
   "source": [
    "### 6.5. Preparar Features e Rótulo (`isFraud`) para o Classificador\n",
    "\n",
    "1. **Selecionar colunas relevantes**:\n",
    "   - Numéricas: \n",
    "     - `amount`, `oldbalanceOrg`, `newbalanceOrig`, `oldbalanceDest`, `newbalanceDest`, `forecast_volume`\n",
    "   - Categóricas:\n",
    "     - `type` (transformar em one-hot: `PAYMENT`, `TRANSFER`, `CASH_OUT`, etc.)\n",
    "   - Features temporais extras:\n",
    "     - Hora do dia: `df.index.hour`  \n",
    "     - Dia da semana: `df.index.dayofweek`  \n",
    "   - Rótulo: `isFraud` (0 ou 1)\n",
    "\n",
    "2. **Normalizar / Escalar (MinMax) os campos numéricos**.\n",
    "\n",
    "3. **Construir um DataFrame final de features** (sem rótulo).  \n",
    "4. **Converter one-hot para `type`** usando `pd.get_dummies(...)`.\n",
    "\n",
    "5. **Separar X (features) e y (rótulo)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcef2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5.1. Extrair features temporais\n",
    "df[\"hour_of_day\"] = df.index.hour\n",
    "df[\"day_of_week\"] = df.index.dayofweek\n",
    "\n",
    "# 6.5.2. One-hot encode para 'type'\n",
    "#           (podemos limitar às categorias mais comuns ou usar todas)\n",
    "df_types = pd.get_dummies(df[\"type\"], prefix=\"type\")\n",
    "\n",
    "# 6.5.3. Montar DataFrame “features_raw”\n",
    "features_raw = pd.concat(\n",
    "    [\n",
    "        df_types,\n",
    "        df[\n",
    "            [\n",
    "                \"amount\",\n",
    "                \"oldbalanceOrg\",\n",
    "                \"newbalanceOrig\",\n",
    "                \"oldbalanceDest\",\n",
    "                \"newbalanceDest\",\n",
    "                \"forecast_volume\",\n",
    "                \"hour_of_day\",\n",
    "                \"day_of_week\",\n",
    "            ]\n",
    "        ],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# 6.5.4. Normalizar somente as colunas numéricas (exceto one-hot)\n",
    "num_cols = [\n",
    "    \"amount\",\n",
    "    \"oldbalanceOrg\",\n",
    "    \"newbalanceOrig\",\n",
    "    \"oldbalanceDest\",\n",
    "    \"newbalanceDest\",\n",
    "    \"forecast_volume\",\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "]\n",
    "scaler_feat = MinMaxScaler(feature_range=(0, 1))\n",
    "features_raw[num_cols] = scaler_feat.fit_transform(features_raw[num_cols])\n",
    "\n",
    "# 6.5.5. Definir X e y\n",
    "X_all = features_raw.values\n",
    "y_all = df[\"isFraud\"].values\n",
    "\n",
    "print(\"Shape de X_all:\", X_all.shape)\n",
    "print(\n",
    "    \"Distribuição de fraude (valor único de y):\", np.unique(y_all, return_counts=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee6e99",
   "metadata": {},
   "source": [
    "### 6.6. Criar Sequências de Transações para LSTM\n",
    "\n",
    "1. **Definir** um `look_back_trans = 10` (por exemplo) que represente quantas transações anteriores queremos que o LSTM “veja” para predizer a transação atual.  \n",
    "2. **Percorrer** todo o DataFrame ordenado e, para cada índice `i ≥ look_back_trans`, construir:  \n",
    "   - `X_seq[i - look_back_trans] = X_all[i - look_back_trans : i]` (shape `(look_back_trans, num_features)`)  \n",
    "   - `y_seq[i - look_back_trans] = y_all[i]`  \n",
    "3. **Dividir** em conjuntos de treino/teste (por ordem cronológica, sem embaralhar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fafc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6.1. Definir look_back para transações\n",
    "look_back_trans = 10\n",
    "\n",
    "# 6.6.2. Construir sequências\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "for i in range(look_back_trans, len(X_all)):\n",
    "    X_seq.append(X_all[i - look_back_trans : i, :])\n",
    "    y_seq.append(y_all[i])\n",
    "\n",
    "X_seq = np.array(X_seq)  # shape: (n_samples, look_back_trans, n_features)\n",
    "y_seq = np.array(y_seq)  # shape: (n_samples,)\n",
    "\n",
    "print(\"Shape X_seq:\", X_seq.shape)\n",
    "print(\"Shape y_seq:\", y_seq.shape)\n",
    "\n",
    "# 6.6.3. Dividir em treino/teste (80% / 20%), preservando ordem temporal\n",
    "n_samples = X_seq.shape[0]\n",
    "train_size = int(n_samples * 0.8)\n",
    "\n",
    "X_train_clf = X_seq[:train_size]\n",
    "y_train_clf = y_seq[:train_size]\n",
    "X_test_clf = X_seq[train_size:]\n",
    "y_test_clf = y_seq[train_size:]\n",
    "\n",
    "print(\"Samples treino:\", X_train_clf.shape[0], \"| Samples teste:\", X_test_clf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082a708",
   "metadata": {},
   "source": [
    "### 6.7. Definir e Compilar o Modelo LSTM para Classificação\n",
    "\n",
    "- Usaremos uma arquitetura simples:\n",
    "  1. Uma ou duas camadas LSTM (com `units=50`, por exemplo),\n",
    "  2. Dropout para evitar overfitting,\n",
    "  3. Uma camada `Dense(1, activation=\"sigmoid\")` para prever probabilidade de fraude.\n",
    "  4. Loss: `binary_crossentropy`; Métrica: `accuracy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a35b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7.1. Definir o modelo\n",
    "model_clf = Sequential()\n",
    "model_clf.add(\n",
    "    LSTM(units=50, input_shape=(look_back_trans, X_all.shape[1]), return_sequences=True)\n",
    ")\n",
    "model_clf.add(Dropout(0.2))\n",
    "model_clf.add(LSTM(units=50, return_sequences=False))\n",
    "model_clf.add(Dropout(0.2))\n",
    "model_clf.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 6.7.2. Compilar\n",
    "model_clf.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a7925",
   "metadata": {},
   "source": [
    "### 6.8. Treinamento do Classificador LSTM\n",
    "\n",
    "- Ajustaremos o modelo em `X_train_clf, y_train_clf`.\n",
    "- Evitaremos embaralhar (`shuffle=False`) para preservar a ordem temporal.\n",
    "- Usaremos validação interna de 10% do conjunto de treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8.1. Treinar\n",
    "\n",
    "# Certifique-se de que os arrays têm dtype numérico adequado\n",
    "X_train_clf = X_train_clf.astype(np.float32)\n",
    "y_train_clf = y_train_clf.astype(np.float32)\n",
    "\n",
    "history_clf = model_clf.fit(\n",
    "    X_train_clf,\n",
    "    y_train_clf,\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False,  # importante manter a ordem temporal\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fe8bb",
   "metadata": {},
   "source": [
    "### 6.9. Avaliação no Conjunto de Teste\n",
    "\n",
    "1. Predizer probabilidades em `X_test_clf`  \n",
    "2. Converter para rótulo binário\n",
    "3. Calcular Acurácia, Matrizes de Confusão e, opcionalmente, AUC-ROC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.9.1. Verificar e corrigir o tipo de dados de X_test_clf\n",
    "print(\"Tipo original de X_test_clf:\", X_test_clf.dtype)\n",
    "print(\"Shape de X_test_clf:\", X_test_clf.shape)\n",
    "\n",
    "X_test_clf_fixed = X_test_clf.astype(np.float32)\n",
    "\n",
    "# 6.9.2. Predição de probabilidade\n",
    "y_pred_prob = model_clf.predict(X_test_clf_fixed, verbose=0).flatten()\n",
    "\n",
    "# Certificar que os arrays são do tipo numérico adequado\n",
    "y_test_clf_num = np.asarray(y_test_clf).astype(np.int32)\n",
    "y_pred_prob_num = np.asarray(y_pred_prob).astype(np.float32)\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5]\n",
    "\n",
    "for thr in thresholds:\n",
    "    # 6.9.3. Converter para 0/1 usando o limiar atual\n",
    "    y_pred = (y_pred_prob_num >= thr).astype(np.int32)\n",
    "\n",
    "    # 6.9.4. Acurácia\n",
    "    acc = accuracy_score(y_test_clf_num, y_pred)\n",
    "\n",
    "    # 6.9.5. Matriz de Confusão\n",
    "    cm = confusion_matrix(y_test_clf_num, y_pred)\n",
    "\n",
    "    print(f\"\\n--- Threshold = {thr:.1f} ---\")\n",
    "    print(f\"Acurácia (test) = {acc:.4f}\")\n",
    "    print(\"Matriz de Confusão (Test):\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(f\"Matriz de Confusão (Test) – Threshold = {thr:.1f}\")\n",
    "    plt.xlabel(\"Previsto\")\n",
    "    plt.ylabel(\"Verdadeiro\")\n",
    "    plt.xticks([0.5, 1.5], [\"Não Fraude (0)\", \"Fraude (1)\"])\n",
    "    plt.yticks([0.5, 1.5], [\"Não Fraude (0)\", \"Fraude (1)\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6.9.6. AUC-ROC\n",
    "auc = roc_auc_score(y_test_clf_num, y_pred_prob_num)\n",
    "print(f\"AUC-ROC (test): {auc:.4f}\")\n",
    "\n",
    "# 6.9.7. Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_clf_num, y_pred_prob_num)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.title(\"Curva ROC (Test)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7a701",
   "metadata": {},
   "source": [
    "### 6.9.7 Análise Comparativa de Thresholds (0.3, 0.4 e 0.5)\n",
    "\n",
    "A seção a seguir compara, para cada limiar (threshold), as principais métricas de avaliação (Acurácia, Precisão, Recall e F1-Score) e mostra a Matriz de Confusão correspondente. Lembre-se de que a AUC-ROC (≈ 0.9253) não muda, pois depende apenas das probabilidades e não do ponto de corte.\n",
    "\n",
    "#### 1. Resultados Numéricos\n",
    "\n",
    "| Threshold |   TP   |   FP   |   FN   |     TN     | Acurácia | Precisão | Recall  | F1-Score |\n",
    "|:---------:|:------:|:------:|:------:|:----------:|:--------:|:--------:|:-------:|:--------:|\n",
    "|  0.30     |  3 133 | 25 194 |  1 121 | 1 243 074  | 0,9795   | 0,1106   | 0,7365  | 0,1920   |\n",
    "|  0.40     |  3 057 | 18 651 |  1 197 | 1 249 617  | 0,9844   | 0,1408   | 0,7186  | 0,2355   |\n",
    "|  0.50     |  2 944 | 12 597 |  1 310 | 1 255 671  | 0,9890   | 0,1899   | 0,6923  | 0,2970   |\n",
    "\n",
    "> **Obs.:**  \n",
    "> - Para threshold = 0.40, usamos valores exatos obtidos pelo código:  \n",
    ">   - **TP = 3 057**, **FP = 18 651**, **FN = 1 197**, **TN = 1 249 617**  \n",
    ">   - **Acurácia ≈ 0,9844**, **Precisão ≈ 0,1408**, **Recall ≈ 0,7186**, **F1-Score ≈ 0,2355**.  \n",
    "> - Esses valores confirmam que threshold = 0.40 fica numericamente entre os thresholds 0.30 e 0.50.\n",
    "\n",
    "##### Cálculo das Métricas\n",
    "\n",
    "1. **Acurácia**  \n",
    "   $$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$  \n",
    "   - *Threshold = 0.30:*  \n",
    "     \\(\\frac{3\\,133 + 1\\,243\\,074}{1\\,272\\,522} \\approx 0{,}9795\\)  \n",
    "   - *Threshold = 0.40:*  \n",
    "     \\(\\frac{3\\,057 + 1\\,249\\,617}{1\\,272\\,522} \\approx 0{,}9844\\)  \n",
    "   - *Threshold = 0.50:*  \n",
    "     \\(\\frac{2\\,944 + 1\\,255\\,671}{1\\,272\\,522} \\approx 0{,}9890\\)\n",
    "\n",
    "2. **Precisão (Precision)**  \n",
    "   $$\\text{Precision} = \\frac{TP}{TP + FP}$$  \n",
    "   - *Threshold = 0.30:*  \n",
    "     \\(\\frac{3\\,133}{3\\,133 + 25\\,194} \\approx 0{,}1106\\)  (≈ 11 %)  \n",
    "   - *Threshold = 0.40:*  \n",
    "     \\(\\frac{3\\,057}{3\\,057 + 18\\,651} \\approx 0{,}1408\\)  (≈ 14 %)  \n",
    "   - *Threshold = 0.50:*  \n",
    "     \\(\\frac{2\\,944}{2\\,944 + 12\\,597} \\approx 0{,}1899\\)  (≈ 19 %)\n",
    "\n",
    "3. **Recall (Sensibilidade / TPR)**  \n",
    "   $$\\text{Recall} = \\frac{TP}{TP + FN}$$  \n",
    "   - *Threshold = 0.30:*  \n",
    "     \\(\\frac{3\\,133}{3\\,133 + 1\\,121} \\approx 0{,}7365\\)  (≈ 74 %)  \n",
    "   - *Threshold = 0.40:*  \n",
    "     \\(\\frac{3\\,057}{3\\,057 + 1\\,197} \\approx 0{,}7186\\)  (≈ 72 %)  \n",
    "   - *Threshold = 0.50:*  \n",
    "     \\(\\frac{2\\,944}{2\\,944 + 1\\,310} \\approx 0{,}6923\\)  (≈ 69 %)\n",
    "\n",
    "4. **F1-Score**  \n",
    "   $$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$  \n",
    "   - *Threshold = 0.30:*  \n",
    "     \\(2 \\times \\frac{0{,}1106 \\times 0{,}7365}{0{,}1106 + 0{,}7365} \\approx 0{,}1920\\)  \n",
    "   - *Threshold = 0.40:*  \n",
    "     \\(2 \\times \\frac{0{,}1408 \\times 0{,}7186}{0{,}1408 + 0{,}7186} \\approx 0{,}2355\\)  \n",
    "   - *Threshold = 0.50:*  \n",
    "     \\(2 \\times \\frac{0{,}1899 \\times 0{,}6923}{0{,}1899 + 0{,}6923} \\approx 0{,}2970\\)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Matrizes de Confusão\n",
    "\n",
    "- **Threshold = 0.30**  \n",
    "  ```\n",
    "              Previsto Não-Fraude (0)    Previsto Fraude (1)\n",
    "  Verdadeiro 0       TN = 1 243 074           FP =  25 194\n",
    "  Verdadeiro 1       FN =    1 121           TP =   3 133\n",
    "  ```\n",
    "\n",
    "- **Threshold = 0.40**  \n",
    "  ```\n",
    "              Previsto Não-Fraude (0)    Previsto Fraude (1)\n",
    "  Verdadeiro 0       TN = 1 249 617           FP =  18 651\n",
    "  Verdadeiro 1       FN =    1 197           TP =   3 057\n",
    "  ```\n",
    "\n",
    "- **Threshold = 0.50**  \n",
    "  ```\n",
    "              Previsto Não-Fraude (0)    Previsto Fraude (1)\n",
    "  Verdadeiro 0       TN = 1 255 671           FP =  12 597\n",
    "  Verdadeiro 1       FN =    1 310           TP =   2 944\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Interpretação dos Trade-offs\n",
    "\n",
    "1. **Threshold baixo (0.30)**  \n",
    "   - **Recall aumenta** (~ 74 %) → capturamos mais fraudes reais (TP sobe de 2 944 → 3 133).  \n",
    "   - **Precisão cai** (~ 11 %) → geramos muitos falsos-positivos (FP pula de 12 597 → 25 194).  \n",
    "   - **Acurácia geral cai** (~ 97,95 % vs. 98,90 %), pois há bem mais TN classificados incorretamente.\n",
    "\n",
    "2. **Threshold intermediário (0.40)**  \n",
    "   - **Recall ≈ 71,86 %** (TP = 3 057, FN = 1 197), portanto maior que 0.50 (≈ 69 %) mas menor que 0.30 (≈ 74 %).  \n",
    "   - **Precisão ≈ 14,08 %** (TP = 3 057, FP = 18 651), numericamente entre 11 % (0.30) e 19 % (0.50).  \n",
    "   - **Acurácia ≈ 98,44 %**, ou seja, perde menos acurácia que 0.30 (97,95 %) e mantém bom equilíbrio entre TN e TP.\n",
    "\n",
    "3. **Threshold mais alto (0.50)**  \n",
    "   - **Precisão ≈ 19 %** → menos falsos-positivos (FP = 12 597).  \n",
    "   - **Recall ≈ 69 %** → 1 310 fraudes passam despercebidas.  \n",
    "   - **Acurácia ≈ 98,90 %**, a mais alta entre os três thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Qual Threshold Escolher?\n",
    "\n",
    "- Se você **prioriza Recall** (evitar qualquer fraude não detectada), aceite **threshold ≲ 0.30** e esteja preparado para muitas verificações manuais.  \n",
    "- Se você **precisa de um meio-termo** (não inundar o time de análise com falsos alarmes), **threshold ≈ 0.40** entrega **Recall ≈ 71,86 %** e **Precisão ≈ 14,08 %**, equilibrando melhor Precisão x Recall.  \n",
    "- Se o **custo de um falso-positivo for muito alto** (por ex., cada “alarme” gera investigação dispendiosa), mantenha **threshold ≥ 0.50** para reduzir FP, embora aceite perder mais fraudes reais (~31 %).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
